---
bibliography: ucscamsms2015.bib
---

```{r cache=FALSE, echo=FALSE}
  knitr::read_chunk('../bin/plot.R')
```

```{r init, echo=FALSE}
```

# Introduction

This is the introduction

Testing nomenclature:

$a=\frac{N}{A}$
\nomenclature{$a$}{The number of angels per unit area}
\nomenclature{$N$}{The number of angels per needle point}
\nomenclature{$A$}{The area of the needle point}

The equation $\sigma = m a$
\nomenclature{$\sigma$}{The total mass of angels per unit area}
\nomenclature{$m$}{The mass of one angel}
follows easily.


# Scoring an Assignment

**Overview**:

* Aim: Assign a numerical value (score) to each candidate solution.
* Use maxi-min criterion

* Utility = Benefit (\$) - Runtime (hrs) x MachineCost (\$/hr)
* Benefit and MachineCost are fixed
* Need an upper bound for job runtime s.t. job will complete by this bound with
95% probability;
* The actual job runtime (sum of runtimes of tasks in job) must be below this
upper bound 95 times out of 100 (this is calibration)
* To get dist. of job runtime:
 * \< 50 tasks/job: use bootstrap sampling of training set samples
 * \>= 50 tasks/job: use Normal approx. (via CLT)
* Do this for all machines in the action space and pick the machine with the
highest utility


## Case 1: < 50 tasks/job
* Runtime dist. generated via bootstrap sampling
* Shape of dist. depends on number of bootstrap samples


Figure \ref{fig:bootstrap-95pct-ub-1processor-1task} shows the 95% upper bound
on runtimes for jobs with 1 task (and processed on a single processor). Figures
\ref{fig:bootstrap-95pct-ub-1processor-10tasks} and
\ref{fig:bootstrap-95pct-ub-1processor-50tasks} show similar bounds for jobs
with 10 and 50 tasks respectively.

``` {r bootstrap-95pct-ub-1processor-1task, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 1 task (runtime distribution generated by bootstrap re-sampling) \\label{fig:bootstrap-95pct-ub-1processor-1task}'}
```

``` {r bootstrap-95pct-ub-1processor-10tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 10 tasks (runtime distribution generated by bootstrap re-sampling \\label{fig:bootstrap-95pct-ub-1processor-10tasks}'}
```

``` {r bootstrap-95pct-ub-1processor-50tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 50 tasks (runtime distribution generated by bootstrap re-sampling \\label{fig:bootstrap-95pct-ub-1processor-50tasks}'}
```

Example of scheduling 3 tasks on 1 instance. The time series of scores for this
example is shown in Figure \ref{fig:sched-3task-2inst}.

``` {r sched-3task-2inst, echo=FALSE, fig.cap='Caption: Schedule 3 tasks on 1 instance \\label{fig:sched-3task-2inst}'}
```

Example of scheduling 1 task on 1 instance. The relevant figure is Figure
\ref{fig:sched-1task-1inst}.

``` {r sched-1task-1inst, echo=FALSE, fig.cap='Caption: Schedule 1 task on 1 instance \\label{fig:sched-1task-1inst}'}
```

## Case 2: > 50 tasks/job
* Runtime dist. generated via Normal approx. (CLT)

Figures \ref{fig:bootstrap-95pct-ub-1processor-100tasks} and
\ref{fig:bootstrap-95pct-ub-1processor-150tasks} shows the 95% upper bound on
runtimes for jobs with 100 tasks (and processed on a single processor).The upper
bound is generated via a Normal approximation to the distribution of job
runtimes (via CLT).

``` {r bootstrap-95pct-ub-1processor-100tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 100 tasks (runtime distribution generated by Normal approximation via CLT \\label{fig:bootstrap-95pct-ub-1processor-100tasks}'}
```

``` {r bootstrap-95pct-ub-1processor-150tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 150 tasks (runtime distribution generated by Normal approximation via CLT \\label{fig:bootstrap-95pct-ub-1processor-150tasks}'}
```

``` {r bootstrap-95pct-ub-1processor-200tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 200 tasks (runtime distribution generated by Normal approximation via CLT \\label{fig:bootstrap-95pct-ub-1processor-200tasks}'}
```


# Finding the optimal assignment

This section is about finding the optimal assignment.

For each cluster + instance type combination:
* Step 1 (optimization): find an 'optimal' assignment under the given
constraints that completes the job at the earliest possible time
* Step 2 (decision): decide if the assignment meets the deadline constraint
(i.e., assignment will allow job to complete by deadline with 95% prob.)

Final decision: choose the assignment with the best score from those
assignments that satisfy the deadline constraints


**Low temperature starts**
Start with a 'good' solution and low temperature so we don't waste time exploring assignments that are worse than the initial assignment.

## Case 1: < 50 tasks/job

## Case 2: > 50 tasks/job

*Validation*

* When runtimes are exponentially distributed, compare with LEPTF
* When runtimes are NOT exponentially distributed, compare with 'truth'
generated by exhaustive search of sample space


# Conclusions and future work

## Conclusions

This is the conclusion

## Future work

The methodology developed above can be extended by the following ways:

### Improved candidate generation
The efficiency of the SA algorithm is strongly influenced by the methods used to generate candidate assignments.
Candidate assignments are currently generated by shifting (moving or exchanging) tasks between processors in a cluster.
The number of tasks shifted, the actual tasks shifted and the processors between which the tasks are shifted are all randomly chosen.
Up to one-third of the tasks assigned to a processor can be moved to or exchanged with another processor (the one-third upper bound is arbitrary threshold and is a tunable parameter of the algorithm).
This can sometimes result in 'lopsided' assignments where some processors are assigned no tasks or very few tasks while other processors are assigned most of the tasks in the job.
For large jobs with tight deadlines, such assignments are very unlikely to complete the job by the deadline and should be rejected before attempting to compute their score.
Since score computation is the most time consuming step in the algorithm, this neighborhood pruning will result in time savings proportional to the number of such assignments rejected.
An initial definition of a 'lopsided' assignment could be one that contains a processor that is assigned less than half the average number of tasks/processor in the best assignment found so far.

The number of tasks shifted between 2 processors is currently independent of the temperature and can vary from 1 to one-third the number of tasks assigned to a processor.
This means that it is possible to generate candidate assignments that are very different from the current best assignment.
Since the number of assignment with low score is much higher than the number of assignments with high score, it is unlikely that such candidate assignments will be accepted when the temperature is very low.
To improve the likelihood of generating assignments with higher probability of being accepted when the temperature is low, the candidate assignments should not vary too much from the current assignment, i.e., the number of tasks shifted between processors should decrease with temperature.

### Skip runtime estimation for small cluster sizes
The runtime for the current scoring algorithm increases as the square of the number of the number of instances in the cluster.
When trying to determine the minimum number of instances in a cluster of a certain instance type that will complete the job by the deadline, we should ignore clusters with too few instances and focus only on cluster sizes that have a reasonable chance of completing the job by the deadline.
Use the min values in the training set for each size and see if the cluster is able to complete them by the deadline.
If not, move on to the next size.

### Re-starting the annealing process
Restarting the cooling schedule from a good solution is a common way to spend more time exploring the neighborhood of the good solution and less time exploring poor solutions.
The current algorithm resets to the best assignment so far if the score of the accepted assignment differs from the score of the best assignment found so far by more than a certain value.
The temperature is not reset and remains at the current value.
This avoids moving to neighborhoods containing assignments that are unlikely to complete the job by the deadline.
Other ways to focus on neighborhoods with high likelihood of containing good assignments include moving back to the best assignment found so far if the assignment score decreases consecutively for several moves, moving back randomly, etc.
The temperature can also be reset to its original value after moving back instead of leaving it at its current value.

### Alternative cooling schedules
In the current cooling schedule, temperature decreases linearly with every iteration, with exactly 1 iteration at each temperature.
Alternative cooling schedules include ones where the temperature decreases exponentially, where there are multiple iterations at each temperature, etc.
The best schedule to use for this problem has to be determined empirically, keeping all other factors like cluster size, job size, initial assignment, maximum temperature, number of iterations, etc. constant.

### Alternative stochastic local search methods
The simulated annealing algorithm [@Kirkpatrick1983] used in Chapter 3 for finding the optimal assignment is just one member of a class of algorithms known as meta-heuristics for solving combinatorial problems.
Other members of this class include tabu search [@Glover1989, @Glover1990], genetic algorithms [@Holland1992], Ant Colony Optimization [@Dorigo2006] and their variants [@Hoos2004].
For tabu search, this can be done by ...
For genetic algorithmd, this can be done by ...
For ant colony optimization, this can be done by ...

### Extension to Spot instances
In the calculation of utility in Eq ???, Cost is defined as job runtime (hrs) $\times$ instance cost (\$/hr).
Instance cost is assumed to be fixed while job runtime is variable.
Spot instances are much cheaper by their cost is variable.
This introduced additional level of uncertainty into the model.
Cost for Spot instances is given as time series data generated from an unknown model.
Need to model the cost effectively and pick a maximum bid price such that the job is not interrupted because the current Spot price exceeds the max bid price.
Can extend this further by using a low bid price with checkpointing and re-processing the task that was interrupted and processing all remaining tasks
Need to consider the possibility that might not get a Spot instance and will have to use an On Demand instance instead for the remaining tasks.

### Variance of runtimes & training set sample sizes
Currently using a fixed number of samples for each task size.
It is possible that different task sizes will have different number of training set samples.
Runtimes for task sizes with large number of training set samples will have lower variance that runtimes for sizes with fewer number of samples.
Also, the same task size can have different number of samples for different instances types.
Similarly, runtimes for a size on an instance with a large number of samples will have lower variance than runtimes for the same size on other instances with fewer number of samples.
Can choose to run task on instance type with fewer number of samples in case it turns out to faster than an instance type with more samples.
Possible use of multi-armed bandits to balance explore vs. exploit here since runtimes for current set of tasks will be added to training set for tasks from the next job.

### Move bootstrap code from R to C++
All code in the `schedulr` R package is in R.
The code to do bootstrap re-sampling takes the longest amount of time.
This code is run for all processors for which the Normal approximation cannot be used to determine distribution of runtime (i.e. when the number of tasks assigned to the processor is less than a certain threshold, currently set at 50).
The runtime of the algorithm currently increases as the square of the number of processors, so for problems with a large number of processors and few tasks/processor, the bootstrap re-sampling code is run many times in each iteration.
This will increase the time taken to find the optimal number of processors and the optimal assignment of tasks to these processors.
Substantial speedup can be obtained by moving the R code for bootstrap re-sampling to C++ and calling the C++ code from the R package via Rcpp.


# Bibliography
