---
bibliography: ucscamsms2015.bib
---

```{r cache=FALSE, echo=FALSE}
  knitr::read_chunk('../bin/plot.R')
```

```{r init, echo=FALSE}
```

# Introduction

## Background on Cloud Computing


A recent trend in the IT industry has been the emergence of *cloud computing* where computing infrastructure is maintained in one or more large data centers and offered for use via a network connection [@Interoute:WhatIsCloudComputing].
The main advantages of such a setup are sharing of resources, economies of scale and flexibility to use as much or as little of the computing infrastructure as needed.
This is in contrast to traditional computing infrastructure setups where clients have to invest in purchasing and maintaining hardware and predict future computing needs well in advance to avoid over-investing or under-investing in their infrastructure.
Other advantages of a cloud computing setup include instant access to a vast and varied amount of computing resources, flexible pricing where the client only pays for the resources used and avoidance of a single point of failure due to the way clients access the computing infrastructure and the redundancies built in to this infrastructure.

Cloud computing offerings can be classified into three broad types.
The first type is Infrastructure-as-a-Service (IaaS) where only basic virtualized environments running on physical hardware are available.
Everything from the operating system upwards must be installed and maintained by the client.
This is useful when the client wants complete control over the software environment but does not want to deal with maintaining hardware or virtualized environments.
The second type of offering is Platform-as-a-service (PaaS) where the cloud service provider offers a virtualized environment with an operating system and common software required by most applications.
The provider takes care of maintaining and updating this software while the client focuses on building custom applications.
The last type is Software-as-a-Service where entire software applications are offered on demand.
In this setup, software applications are *rented* from the provider and are accessed over a network connection instead of being purchased and installed on the client's computing infrastructure.
In all the above cases, the hardware is usually shared across multiple clients and access to hardware is abstracted away from the client.

Cloud computing offerings can also be classified as *private clouds* or *public clouds*.
Private clouds are virtualized environments that are meant for the use of a single client and are typically owned by the client.
They cannot be accessed by anyone else.
A public cloud, in contrast, is owned and operated by a cloud service provider and is accessed via a public network like the internet.
It can be utilized by anyone who wants to rent infrastructure or software on demand.
Examples of public clouds include Amazon Web Services from Amazon.com, Inc., Azure from Microsoft, Inc. and Google Cloud Platform from Google, Inc.
This work deals only with PaaS offerings from public clouds, though the findings are equally applicable to private clouds.

The availability of a large amount of computing resources on demand and ability to pay for these resources by the hour has enabled the migration of many workloads to a cloud computing environment [@RightScale:2015].
These workloads can be processed in parallel on multiple resources and can complete much quicker than if they were processed on a single resource.
While there is no practical constraint on the number of resources that can be used, budgetary constraints limit the number and type of resources that can be used to process a workload.
Thus there is always a trade-off between the benefit arising from processing a workload quickly and cost of resources used to process the workload.

In most cases, the benefit is a fixed value and does not vary with processing time.
So minimizing the cost of processing will maximize the benefit to the client.
Efficient scheduling methods help with scheduling the tasks in a workload such that the total processing cost is minimized.
The rest of this work deals with developing efficient methods of scheduling tasks to minimize processing costs and maximize benefit.


## Background on Scheduling

Scheduling refers to the process of assigning tasks to resources that complete the tasks.
It is a very common problem and arises in a variety of everyday situations such as scheduling manufacturing tasks in factories, scheduling takeoffs and landings of planes in airports, scheduling sports tournaments, scheduling nurse shifts in hospitals, scheduling tasks across multiple instances, etc. [@Hoos2004].

The input to a scheduling problem is a set of tasks to be completed and a set of resources that can process the tasks.
The output is a schedule that maps tasks to resources in a certain order.
Scheduling problems are typically subject to various feasibility constraints that define a valid schedule.
Examples of such constraints include maintaining minimum time intervals between plane takeoffs and landings, avoiding adjacent shifts for nurses, completing tasks by a certain deadline, using a fixed number of machines, etc.
Once a set of feasible schedules is available, various optimization criteria are used select the best schedule from this set.
Examples of such criteria include minimizing total weighted delay over all flights, preference of nurses for certain shifts, minimizing maximum completion time for tasks, etc.

Scheduling problems can be classified in various ways based on task characteristics, resource characteristics and optimization criteria [@Hoos2004, @Pinedo2012]. Properties of tasks include pre-emption, release times, deadlines, dependencies, weights, amount of pre-processing, etc.

*Pre-emption* refers to the interruption of a task in progress so that it can be re-started later on the same or different resource.
*Pre-emptive* schedules allow tasks to be interrupted while *non-pre-emptive schedules* do not allow pre-emption - once a task has started processing, it must complete processing on the same resource.

Tasks can be associated with release times and deadlines.
Release time for a task represents the earliest time a task is available for processing.
A task cannot start processing before its release time.
A deadline represents the time by which a task must complete processing.
When a set of tasks is associated with a deadline, all tasks in the set must complete processing by the deadline.
Similarly, when a set of tasks has a release time, none of the tasks can begin processing before the release time.

Dependencies between tasks control the order in which tasks can be processed.
Examples of dependencies include precedence constraints between tasks where one task depends on other tasks and cannot start processing until all tasks on which it is dependent have completed.
Task weights and the amount of pre-processing tasks have received also influence the types of schedules that can be generated.
Tasks with higher weights must usually be processed first and tasks that have been pre-processed typically require less processing time.

Scheduling problems can also be classified by the properties of resources.
Tasks can be processed on a single resource or by multiple resources in parallel.
When using resources in parallel, the resources can be *identical* or *unrelated*.
Identical resources are all of the same type; task runtime will be the same regardless of which resource is used to process the task.
When using unrelated resources in parallel, runtime for a task depends on the resource used to process the task and might vary for each resource.

Finally, scheduling problems can be classified by the the type of optimization criteria used to select the best schedule from a set of feasible schedules.
Examples of such criteria include minimizing the maximum completion time (also known as *makespan*) of a set of tasks, minimizing the weighted completion time of a set of tasks where each task is assigned a weight proportional to its benefit, minimizing the maximum delay of a set of tasks for tasks with deadlines, etc.


\nomenclature{$\mathcal{T}$}{Set of input tasks that must be processed}
\nomenclature{$\mathcal{B}$}{Benefit (specified in dollars) paid out when all tasks in $T$ are completed by the deadline. If {$\mathcal{T}$} is not complete by the deadline, $\mathcal{B}=\$0$.}
\nomenclature{$\mathcal{D}$}{Deadline (specified in seconds) by which all tasks in $\mathcal{T}$ must complete processing}
\nomenclature{$\mathcal{I}$}{Set of instances available to process tasks}
\nomenclature{$L_i$}{Length of task $T_i$}
\nomenclature{$R_i,j$}{Runtime of task $T_i$ on instance $I_j$; known only when $T_i$ completes processing on $I_j$; all instances of the same type are assumed to process the task in the same amount of time}
\nomenclature{$R_j$}{Total runtime of all tasks assigned to instance $I_j$}
\nomenclature{$\mathcal{M}$}{Makespan for the set of tasks $\mathcal{T}$}
\nomenclature{$\mathcal{U}$}{Utility}
\nomenclature{$\mathcal{C}_j$}{Cost per hour of instance $I_j$ (specified in dollars)}


## Problem Formulation

The previous section highlights the need for efficient scheduling methods to complete a set of tasks on time and within budget, especially when the runtimes are distributed according to unknown probability distributions.
Each schedule is associated with a *cost* that must be paid to the data center for use of their computing resources.
This cost depends on the amount of time the resources are used and is determined by the *total time* taken by the schedule to complete all input tasks.
Given a set of *feasible* schedules that can complete the set of tasks by the deadline, finding the schedule in this set that completes the task in the shortest amount of time (and therefore the lowest cost) represents an *optimization problem*.

An important characteristic of scheduling problems is that the number of feasible schedules is usually very large.
So it is not possible to find the globally optimal schedule in a reasonable amount of time using brute force methods.
In fact, most scheduling problems are considered to be NP-hard [@Garey1979] and form one of the most important classes of combinatorial problems.
This leads to the use of various heuristics and stochastic optimization methods to find, in a reasonable amount of time, a schedule that is 'close enough' to the global optimum.

In this work, we focus on the specific problem of finding optimal non-pre-emptive schedules that minimize makespan while processing a set of tasks on multiple identical resources in parallel. All tasks have the same weight, have not undergone any pre-processing and are immediately available for processing.

* Easier to do when the runtimes are known in advance
* Harder to do when runtimes are random and their distribution is unknown
* Using Bayesian decision theory to find optimal schedule this via maximizing expected utility


Let $\mathcal{T} := {T_1, T_2, \dots T_n}$ be the set of input tasks that must be processed.
This set of tasks is associated with a benefit $\mathcal{B}$ specified in dollars and a deadline $\mathcal{D}$ specified in seconds.
Each task $T_i$ is independent of all other tasks in $\mathcal{T}$; so tasks can be processed in any order.
$\mathcal{T}$ is considered to complete when all tasks in $\mathcal{T}$ are complete.

Let $I := {I_1, I_2, \dots I_m}$ be the set of resources (also known as instances in this work) available to process tasks in $\mathcal{T}$.
These instances are grouped into various instance types depending on their configuration.
Let $r_{i,j}$ be the runtime of task $T_i$ on instance $I_j$.
Time taken to complete a task depends on the type of instance on which the task is processed; slower instances will take more time to process the same task compared to faster instances.

Let $J_j$ be the set of tasks assigned to instance $I_j$.
Then $R_j := \sum_{k \in J_j} r_{k,j}$ is the total runtime for all jobs assigned to instance $I_j$.
The maximum runtime across all instances or **makespan** $\mathcal{M}$ of $\mathcal{T}$ is defined as:

\begin{equation}\label{eq:makespandef}
\mathcal{M} = \max_{j \in \mathcal{I}} {R_j}
\end{equation}


The problem of minimizing makespan has been studied extensively [include refs here] but it cannot be solved in polynomial time because the problem is NP-hard even for the simple case of processing tasks on 2 identical instances.
However, polynomial time approximation schemes have been developed to provide an approximate solution to this problem. One such scheme is a heuristic  called Longest Processing Time First.
This heuristic first orders tasks in decreasing order of runtimes and then assigns the largest  task that has not yet started processing to the next available machine.
This scheme has been shown to minimize makespan for tasks whose actual runtimes are known in advance [@Pinedo2012].

* Maximizing utility
* Benefit and MachineCost are fixed

\begin{equation}\label{eq:utilitydef}
	\mathcal{U} = \mathcal{B} - \left(\sum_{i}R_{i,j}\right) \times \mathcal{C}_j
\end{equation}

But task runtimes $r_{i,j}$ are not always known in advance.
It is not uncommon to come across problems where runtime is known only when the task completes processing.
But the probability distribution of runtimes is known before processing starts and PTAS have been developed to find approximate solutions to such problems.
When the runtimes are exponentially distributed, the Longest *Expected* Processing Time First heuristic has been shown to minimize the *expected* makespan [@Pinedo2012].
This heuristic first sorts tasks in decreasing order of expected runtime and then assigns the largest task that has not yet started processing to the next available machine. The proof of this heuristic uses the memory-less property of exponential distributions and hence this heuristic is not applicable to cases where the runtimes are not exponentially distributed.

A major contribution of the current work is to present a method to minimize makespan that does not make any assumptions about the distribution of task runtimes.

* Maximizing expected utility
* equation for maximizing expected utility
* decision theory

\begin{equation}\label{eq:utilitydef}
	\mathcal{U} = \mathcal{B} - \left(\sum_{i}R_{i,j}\right) \times \mathcal{C}_j
\end{equation}

Utility = Benefit (\$) - Runtime (hrs) x MachineCost (\$/hr)
* Benefit and MachineCost are fixed


## Data

The data for this problem comprises the following:

* **Instance types:**
Each Amazon data center has hundreds of thousands of virtual machines  (also known as instances) (ref to EC2 website) running on physical hardware.
Each instance is associated with a certain configuration of instance type and speed, memory, local disk space, network speed, etc.
This configuration of resources represents an instance type.
Examples of instance types include compute-optimized instance types with faster, more powerful instances, memory-optimized instance types with large amounts of memory, storage-optimized instance types with large amounts of local disk space, etc.
There are dozens of such instance types and thousands of instances of the same type in each data center.
The instance types represent the action space of the decision problem with selection of an instance type to process input types being the corresponding action.
In this work, we assume that tasks are constrained only by instance type and speed and not by any other resource.

* **Task lengths**:
The input to the problem is a set of tasks.
Each task is associated with a property called *length*.
Runtime for a task is roughly proportional to its length.
Besides length, other properties of the task also influence task runtime.
If these properties were easy to quantify, then runtime would be deterministic function of length and the other properties.
But these properties are not easily quantifiable, so task runtime is considered to be a random variable with length as the only co-variate that can be used to predict runtime.

* **Training set samples:**
In addition to task length, task runtime also depends on the instance type on which the task is processed.
Distributions for task runtimes follow arbitrary unknown distributions that cannot be expressed in closed form.
Further, runtimes for different task lengths can follow potentially different unknown distributions.
So runtime distributions for a given combination of task length and instance type are determined empirically by running multiple tasks of the same length on the instance type and recording the actual runtime for each task.
These runtimes form the *training set* for the above combination and are used to predict runtimes for future tasks with the same length on the same instance type.
It is not practical to obtain a training set sample for all possible task lengths, so a representative sample of 83 task lengths was selected and 200 training set runtimes were generated for each of these lengths.

For the purposes of validation, two separate training sets were generated for each combination of task length and instance type.
In the first set, the runtimes follow an exponential distribution while in the other set, they follow a Normal distribution.


# Scoring an Assignment

Finding an optimal assignment for a set of tasks requires the comparison of assignments to determine the *best* assignment.
This comparison requires that each assignment have an associated property (which we call score) that is determined in the same way and with reference to the same baseline for all assignments.
In this chapter, we develop the definition of such a property and show that it can be used to select the best assignment from a set of assignments.

Each set of tasks is associated with a benefit $B$ (specified in units of dollars) and a deadline $D$ (specified in units of time).
The benefit is paid out only when the all tasks are completed by the deadline. The benefit is $\$0$ if the tasks are not completed by the deadline.
This is a constraint imposed by the submitter of the tasks and leads to a definition of score where score = the probability of the assignment allowing all tasks to be completed by the deadline.
Thus the scoring function maps an assignment to a value in $(0,1)$ called the score.
The higher the value of the score, the higher the probability of completing all tasks by the deadline and the higher the probability of getting the benefit.
Since all assignments are scored against the same deadline, assignment score can be used to compare candidate assignments and select the assignment with the highest probability of getting the benefit.

As mentioned in the previous chapter, we will use makespan as the optimization criterion to select the best assignment from a set of feasible assignments.
This chapter will focus on scoring assignments for a single instance and the next chapter will deal with scoring assignments when multiple identical instances are used in parallel.

For a single instance, makespan is the sum of actual runtimes of each task in the input set of tasks and is invariant to the order in which the tasks are processed.
The action space here is the set of instance types available in the data center and the action is the selection of the instance type that maximizes the expected utility as specified in Eq \ref{eq:utilitydef}.

In Eq \ref{eq:utilitydef}, utility is averaged over the distribution of runtimes for each task. But these distributions are unknown and do not have a closed form expression. So they are determined empirically for each combination of task length and instance type by processing multiple tasks with the same length on the given instance type and recording the runtimes. The process is repeated for all task lengths of interest and series of empirical distributions are obtained.

The nature of the runtime distribution depends on the number of tasks whose runtimes are being summed.


**Overview**:

* Aim: Assign a numerical value (score) to each candidate solution.
* Use maxi-min criterion

* Need an upper bound for job runtime s.t. job will complete by this bound with
95% probability;
* The actual job runtime (sum of runtimes of tasks in job) must be below this
upper bound 95 times out of 100 (this is calibration)
* To get dist. of job runtime:
 * \< 50 tasks/job: use bootstrap sampling of training set samples
 * \>= 50 tasks/job: use Normal approx. (via CLT)
* Do this for all machines in the action space and pick the machine with the
highest utility

* Runtime dist. generated via bootstrap sampling
* Shape of dist. depends on number of bootstrap samples

##  Validation: <= 100 tasks/instance

Figure \ref{fig:bootstrap-95pct-ub-1processor-1task} shows the 95% upper bound
on runtimes for jobs with 1 task (and processed on a single processor). Figures
\ref{fig:bootstrap-95pct-ub-1processor-10tasks} and
\ref{fig:bootstrap-95pct-ub-1processor-50tasks} show similar bounds for jobs
with 10 and 50 tasks respectively.

``` {r bootstrap-95pct-ub-1processor-1task, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 1 task (runtime distribution generated by bootstrap re-sampling) \\label{fig:bootstrap-95pct-ub-1processor-1task}'}
```

``` {r bootstrap-95pct-ub-1processor-10tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 10 tasks (runtime distribution generated by bootstrap re-sampling) \\label{fig:bootstrap-95pct-ub-1processor-10tasks}'}
```

``` {r bootstrap-95pct-ub-1processor-50tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 50 tasks (runtime distribution generated by bootstrap re-sampling) \\label{fig:bootstrap-95pct-ub-1processor-50tasks}'}
```

Example of scheduling 3 tasks on 1 instance. The time series of scores for this
example is shown in Figure \ref{fig:sched-3task-2inst}.

``` {r sched-3task-2inst, echo=FALSE, fig.cap='Caption: Schedule 3 tasks on 1 instance \\label{fig:sched-3task-2inst}'}
```

Example of scheduling 1 task on 1 instance. The relevant figure is Figure
\ref{fig:sched-1task-1inst}.

``` {r sched-1task-1inst, echo=FALSE, fig.cap='Caption: Schedule 1 task on 1 instance \\label{fig:sched-1task-1inst}'}
```

##  Validation: > 100 tasks/instance
* Runtime dist. generated via Normal approx. (CLT)

Figures \ref{fig:bootstrap-95pct-ub-1processor-100tasks} and
\ref{fig:bootstrap-95pct-ub-1processor-150tasks} shows the 95% upper bound on
runtimes for jobs with 100 tasks (and processed on a single processor).The upper
bound is generated via a Normal approximation to the distribution of job
runtimes (via CLT).

``` {r bootstrap-95pct-ub-1processor-100tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 100 tasks (runtime distribution generated by Normal approximation via CLT \\label{fig:bootstrap-95pct-ub-1processor-100tasks}'}
```

``` {r bootstrap-95pct-ub-1processor-150tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 150 tasks (runtime distribution generated by Normal approximation via CLT \\label{fig:bootstrap-95pct-ub-1processor-150tasks}'}
```

``` {r bootstrap-95pct-ub-1processor-200tasks, echo=FALSE, message=FALSE, fig.cap='95% upper bound on runtimes for jobs with 200 tasks (runtime distribution generated by Normal approximation via CLT \\label{fig:bootstrap-95pct-ub-1processor-200tasks}'}
```


# Finding an optimal assignment

This section is about finding the optimal assignment using Simulated Annealing.

``` {r sched-113task-4inst, echo=FALSE, message=FALSE, fig.cap='Time series plot showing scores of accepted assignments \\label{fig:sched-113task-4inst}'}
```

For each cluster + instance type combination:
* Step 1 (optimization): find an 'optimal' assignment under the given
constraints that completes the job at the earliest possible time
* Step 2 (decision): decide if the assignment meets the deadline constraint
(i.e., assignment will allow job to complete by deadline with 95% prob.)

Final decision: choose the assignment with the best score from those
assignments that satisfy the deadline constraints


**Low temperature starts**
Start with a 'good' solution and low temperature so we don't waste time exploring assignments that are worse than the initial assignment.

## Simulated Annealing

## Validation

* When runtimes are exponentially distributed, compare with LEPTF
* When runtimes are NOT exponentially distributed, compare with 'truth'
generated by exhaustive search of sample space


# Conclusions and future work

## Conclusions

This is the conclusion

## Future work

The methodology developed above can be extended by the following ways:

### Improved candidate generation
The efficiency of the SA algorithm is strongly influenced by the methods used to generate candidate assignments.
Candidate assignments are currently generated by shifting (moving or exchanging) tasks between processors in a cluster.
The number of tasks shifted, the actual tasks shifted and the processors between which the tasks are shifted are all randomly chosen.
Up to one-third of the tasks assigned to a processor can be moved to or exchanged with another processor (the one-third upper bound is arbitrary threshold and is a tunable parameter of the algorithm).
This can sometimes result in 'lopsided' assignments where some processors are assigned no tasks or very few tasks while other processors are assigned most of the tasks in the job.
For large jobs with tight deadlines, such assignments are very unlikely to complete the job by the deadline and should be rejected before attempting to compute their score.
Since score computation is the most time consuming step in the algorithm, this neighborhood pruning will result in time savings proportional to the number of such assignments rejected.
An initial definition of a 'lopsided' assignment could be one that contains a processor that is assigned less than half the average number of tasks/processor in the best assignment found so far.

The number of tasks shifted between 2 processors is currently independent of the temperature and can vary from 1 to one-third the number of tasks assigned to a processor.
This means that it is possible to generate candidate assignments that are very different from the current best assignment.
Since the number of assignment with low score is much higher than the number of assignments with high score, it is unlikely that such candidate assignments will be accepted when the temperature is very low.
To improve the likelihood of generating assignments with higher probability of being accepted when the temperature is low, the candidate assignments should not vary too much from the current assignment, i.e., the number of tasks shifted between processors should decrease with temperature.

### Skip runtime estimation for small cluster sizes
The runtime for the current scoring algorithm increases as the square of the number of the number of instances in the cluster.
When trying to determine the minimum number of instances in a cluster of a certain instance type that will complete the job by the deadline, we should ignore clusters with too few instances and focus only on cluster sizes that have a reasonable chance of completing the job by the deadline.
Use the min values in the training set for each size and see if the cluster is able to complete them by the deadline.
If not, move on to the next size.

### Re-starting the annealing process
Restarting the cooling schedule from a good solution is a common way to spend more time exploring the neighborhood of the good solution and less time exploring poor solutions.
The current algorithm resets to the best assignment so far if the score of the accepted assignment differs from the score of the best assignment found so far by more than a certain value.
The temperature is not reset and remains at the current value.
This avoids moving to neighborhoods containing assignments that are unlikely to complete the job by the deadline.
Other ways to focus on neighborhoods with high likelihood of containing good assignments include moving back to the best assignment found so far if the assignment score decreases consecutively for several moves, moving back randomly, etc.
The temperature can also be reset to its original value after moving back instead of leaving it at its current value.

### Alternative cooling schedules
In the current cooling schedule, temperature decreases linearly with every iteration, with exactly 1 iteration at each temperature.
Alternative cooling schedules include ones where the temperature decreases exponentially, where there are multiple iterations at each temperature, etc.
The best schedule to use for this problem has to be determined empirically, keeping all other factors like cluster size, job size, initial assignment, maximum temperature, number of iterations, etc. constant.

### Alternative stochastic local search methods
The simulated annealing algorithm [@Kirkpatrick1983] used in Chapter 3 for finding the optimal assignment is just one member of a class of algorithms known as meta-heuristics for solving combinatorial problems.
Other members of this class include tabu search [@Glover1989, @Glover1990], genetic algorithms [@Holland1992], Ant Colony Optimization [@Dorigo2006] and their variants [@Hoos2004].
For tabu search, this can be done by ...
For genetic algorithmd, this can be done by ...
For ant colony optimization, this can be done by ...

### Extension to Spot instances
In the calculation of utility in Eq ???, Cost is defined as job runtime (hrs) $\times$ instance cost (\$/hr).
Instance cost is assumed to be fixed while job runtime is variable.
Spot instances are much cheaper by their cost is variable.
This introduced additional level of uncertainty into the model.
Cost for Spot instances is given as time series data generated from an unknown model.
Need to model the cost effectively and pick a maximum bid price such that the job is not interrupted because the current Spot price exceeds the max bid price.
Can extend this further by using a low bid price with checkpointing and re-processing the task that was interrupted and processing all remaining tasks
Need to consider the possibility that might not get a Spot instance and will have to use an On Demand instance instead for the remaining tasks.

### Variance of runtimes & training set sample sizes
Currently using a fixed number of samples for each task size.
It is possible that different task sizes will have different number of training set samples.
Runtimes for task sizes with large number of training set samples will have lower variance that runtimes for sizes with fewer number of samples.
Also, the same task size can have different number of samples for different instances types.
Similarly, runtimes for a size on an instance with a large number of samples will have lower variance than runtimes for the same size on other instances with fewer number of samples.
Can choose to run task on instance type with fewer number of samples in case it turns out to faster than an instance type with more samples.
Possible use of multi-armed bandits to balance explore vs. exploit here since runtimes for current set of tasks will be added to training set for tasks from the next job.

### Move bootstrap code from R to C++
All code in the `schedulr` R package is in R.
The code to do bootstrap re-sampling takes the longest amount of time.
This code is run for all processors for which the Normal approximation cannot be used to determine distribution of runtime (i.e. when the number of tasks assigned to the processor is less than a certain threshold, currently set at 50).
The runtime of the algorithm currently increases as the square of the number of processors, so for problems with a large number of processors and few tasks/processor, the bootstrap re-sampling code is run many times in each iteration.
This will increase the time taken to find the optimal number of processors and the optimal assignment of tasks to these processors.
Substantial speedup can be obtained by moving the R code for bootstrap re-sampling to C++ and calling the C++ code from the R package via Rcpp.


# Bibliography
